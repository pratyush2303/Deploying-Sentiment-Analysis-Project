{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratyush2303/Deploying-Sentiment-Analysis-Project/blob/main/SageMaker_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZGXvRXjGIsB"
      },
      "source": [
        "# Creating a Sentiment Analysis Web App\n",
        "## Using PyTorch and SageMaker\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## General Outline\n",
        "\n",
        "Recall the general outline for SageMaker projects using a notebook instance.\n",
        "\n",
        "1. Download or otherwise retrieve the data.\n",
        "2. Process / Prepare the data.\n",
        "3. Upload the processed data to S3.\n",
        "4. Train a chosen model.\n",
        "5. Test the trained model (typically using a batch transform job).\n",
        "6. Deploy the trained model.\n",
        "7. Use the deployed model.\n",
        "\n",
        "First, I will not be testing the model in its own step. I will still be testing the model, however, I will do it by deploying your model and then using the deployed model by sending the test data to it. One of the reasons for doing this is so that I can make sure that your deployed model is working correctly before moving forward.\n",
        "\n",
        "In addition, I will deploy and use your trained model a second time. In the second iteration I will customize the way that my trained model is deployed by including some of my own code. In addition, my newly deployed model will be used in the sentiment analysis web app."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TiY1-zMGIsD"
      },
      "source": [
        "## Step 1: Downloading the data\n",
        "\n",
        "I will be using the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqVXxl_SGIsE",
        "outputId": "17af135a-2300-46e9-d863-6966284bc265"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘../data’: File exists\n",
            "--2020-09-18 03:50:04--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
            "\n",
            "../data/aclImdb_v1. 100%[===================>]  80.23M  23.6MB/s    in 4.6s    \n",
            "\n",
            "2020-09-18 03:50:09 (17.4 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%mkdir ../data\n",
        "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "audTKaCEGIsG"
      },
      "source": [
        "## Step 2: Preparing and Processing the data\n",
        "\n",
        "I will be doing some initial data processing. To begin with, I will read in each of the reviews and combine them into a single input structure. Then, I will split the dataset into a training set and a testing set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VSP2xrqfGIsG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "def read_imdb_data(data_dir='../data/aclImdb'):\n",
        "    data = {}\n",
        "    labels = {}\n",
        "\n",
        "    for data_type in ['train', 'test']:\n",
        "        data[data_type] = {}\n",
        "        labels[data_type] = {}\n",
        "\n",
        "        for sentiment in ['pos', 'neg']:\n",
        "            data[data_type][sentiment] = []\n",
        "            labels[data_type][sentiment] = []\n",
        "\n",
        "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
        "            files = glob.glob(path)\n",
        "\n",
        "            for f in files:\n",
        "                with open(f) as review:\n",
        "                    data[data_type][sentiment].append(review.read())\n",
        "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
        "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
        "\n",
        "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
        "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
        "\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxDdgToeGIsG",
        "outputId": "b7f5cbed-0a46-45ce-cb26-2f4d096a58b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
          ]
        }
      ],
      "source": [
        "data, labels = read_imdb_data()\n",
        "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
        "            len(data['train']['pos']), len(data['train']['neg']),\n",
        "            len(data['test']['pos']), len(data['test']['neg'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cokgt-snGIsG"
      },
      "source": [
        "Now that the raw training and testing data has been read from the downloaded dataset, I will combine the positive and negative reviews and shuffle the resulting records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGO1JzlWGIsH"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def prepare_imdb_data(data, labels):\n",
        "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
        "\n",
        "    #Combine positive and negative reviews and labels\n",
        "    data_train = data['train']['pos'] + data['train']['neg']\n",
        "    data_test = data['test']['pos'] + data['test']['neg']\n",
        "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
        "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
        "\n",
        "    #Shuffle reviews and corresponding labels within training and test sets\n",
        "    data_train, labels_train = shuffle(data_train, labels_train)\n",
        "    data_test, labels_test = shuffle(data_test, labels_test)\n",
        "\n",
        "    # Return a unified training data, test data, training labels, test labets\n",
        "    return data_train, data_test, labels_train, labels_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L65HHjuGIsH",
        "outputId": "84940232-ca4d-4f5f-e421-52d2d4536c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMDb reviews (combined): train = 25000, test = 25000\n"
          ]
        }
      ],
      "source": [
        "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
        "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyZMRN5hGIsH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOHnbx3DGIsH",
        "outputId": "3d5c2de6-36ea-4364-ce33-79e42d41815f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "** HERE BE SPOILERS **<br /><br />The government has continued to develop the UniversalSoldier program, now called UniSol. The soldiers are now stronger and are able to take more damage than before. However the government is downsizing, the project endangered and the supercomputer that is in the middle of all feel threatened, so he takes steps to ensure his own safety. He activates and controls the UniSols and start to run mayhem. The only one who can stop them is Deveraux (Van Damme). <br /><br />This movie is about one thing. Choreographed fighting. The story is bad, and is soon drowned in all fights. Whatever happens, and wherever they go, they fight. Unfortunately for this movie, it is no fun watching a fight where you know one part of it is indestructible. Normally you're pretty sure the hero will win, but you still want to feel the fights are between two somewhat equal combatants. Not where one is indestructible and can't lose. Then the fights just become a tool to stretch time. You wait until the final fight when Deveraux miraculously finds a way to beat his unbeatable foes. To further lower my opinion, a desperate and sure sign of a bad movie is how much scantily clad women there are. Well, there aren't really that lot of them, because the characters are most men (there are at least one woman UniSol though), but almost every woman is needlessly shown with at least just a bra once. The female leads get by with this, but we also pass through a strip-club (to use a computer no less) with much more undressed women. These moments do not give anything to the story and is just there to try to please the adolescent-minded male audience.<br /><br />So, in conclusion, boring fights. No more, no less. Well, maybe less...<br /><br />2/10\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(train_X[100])\n",
        "print(train_y[100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYxSBv21GIsI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-XmjqXuGIsI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def review_to_words(review):\n",
        "    nltk.download(\"stopwords\", quiet=True)\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
        "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
        "    words = text.split() # Split string into words\n",
        "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
        "    words = [PorterStemmer().stem(w) for w in words] # stem\n",
        "\n",
        "    return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEzYbkGyGIsI"
      },
      "source": [
        "The `review_to_words` method defined above uses `BeautifulSoup` to remove any html tags that appear and uses the `nltk` package to tokenize the reviews. As a check to ensure try applying `review_to_words` to one of the reviews in the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQq1i3TuGIsJ",
        "outputId": "8df8ed38-65b3-4f90-b8e2-39c755a66c41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['spoiler',\n",
              " 'govern',\n",
              " 'continu',\n",
              " 'develop',\n",
              " 'universalsoldi',\n",
              " 'program',\n",
              " 'call',\n",
              " 'unisol',\n",
              " 'soldier',\n",
              " 'stronger',\n",
              " 'abl',\n",
              " 'take',\n",
              " 'damag',\n",
              " 'howev',\n",
              " 'govern',\n",
              " 'downsiz',\n",
              " 'project',\n",
              " 'endang',\n",
              " 'supercomput',\n",
              " 'middl',\n",
              " 'feel',\n",
              " 'threaten',\n",
              " 'take',\n",
              " 'step',\n",
              " 'ensur',\n",
              " 'safeti',\n",
              " 'activ',\n",
              " 'control',\n",
              " 'unisol',\n",
              " 'start',\n",
              " 'run',\n",
              " 'mayhem',\n",
              " 'one',\n",
              " 'stop',\n",
              " 'deveraux',\n",
              " 'van',\n",
              " 'damm',\n",
              " 'movi',\n",
              " 'one',\n",
              " 'thing',\n",
              " 'choreograph',\n",
              " 'fight',\n",
              " 'stori',\n",
              " 'bad',\n",
              " 'soon',\n",
              " 'drown',\n",
              " 'fight',\n",
              " 'whatev',\n",
              " 'happen',\n",
              " 'wherev',\n",
              " 'go',\n",
              " 'fight',\n",
              " 'unfortun',\n",
              " 'movi',\n",
              " 'fun',\n",
              " 'watch',\n",
              " 'fight',\n",
              " 'know',\n",
              " 'one',\n",
              " 'part',\n",
              " 'indestruct',\n",
              " 'normal',\n",
              " 'pretti',\n",
              " 'sure',\n",
              " 'hero',\n",
              " 'win',\n",
              " 'still',\n",
              " 'want',\n",
              " 'feel',\n",
              " 'fight',\n",
              " 'two',\n",
              " 'somewhat',\n",
              " 'equal',\n",
              " 'combat',\n",
              " 'one',\n",
              " 'indestruct',\n",
              " 'lose',\n",
              " 'fight',\n",
              " 'becom',\n",
              " 'tool',\n",
              " 'stretch',\n",
              " 'time',\n",
              " 'wait',\n",
              " 'final',\n",
              " 'fight',\n",
              " 'deveraux',\n",
              " 'miracul',\n",
              " 'find',\n",
              " 'way',\n",
              " 'beat',\n",
              " 'unbeat',\n",
              " 'foe',\n",
              " 'lower',\n",
              " 'opinion',\n",
              " 'desper',\n",
              " 'sure',\n",
              " 'sign',\n",
              " 'bad',\n",
              " 'movi',\n",
              " 'much',\n",
              " 'scantili',\n",
              " 'clad',\n",
              " 'women',\n",
              " 'well',\n",
              " 'realli',\n",
              " 'lot',\n",
              " 'charact',\n",
              " 'men',\n",
              " 'least',\n",
              " 'one',\n",
              " 'woman',\n",
              " 'unisol',\n",
              " 'though',\n",
              " 'almost',\n",
              " 'everi',\n",
              " 'woman',\n",
              " 'needlessli',\n",
              " 'shown',\n",
              " 'least',\n",
              " 'bra',\n",
              " 'femal',\n",
              " 'lead',\n",
              " 'get',\n",
              " 'also',\n",
              " 'pass',\n",
              " 'strip',\n",
              " 'club',\n",
              " 'use',\n",
              " 'comput',\n",
              " 'less',\n",
              " 'much',\n",
              " 'undress',\n",
              " 'women',\n",
              " 'moment',\n",
              " 'give',\n",
              " 'anyth',\n",
              " 'stori',\n",
              " 'tri',\n",
              " 'pleas',\n",
              " 'adolesc',\n",
              " 'mind',\n",
              " 'male',\n",
              " 'audienc',\n",
              " 'conclus',\n",
              " 'bore',\n",
              " 'fight',\n",
              " 'less',\n",
              " 'well',\n",
              " 'mayb',\n",
              " 'less',\n",
              " '2',\n",
              " '10']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Apply review_to_words to a review (train_X[100] or any other review)\n",
        "review_to_words(train_X[100])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NglUooQ8GIsJ"
      },
      "source": [
        "The method below applies the `review_to_words` method to each of the reviews in the training and testing datasets. In addition it caches the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2Te4KC2GIsJ"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "cache_dir = os.path.join(\"../cache\", \"sentiment_analysis\")  # where to store cache files\n",
        "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
        "\n",
        "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
        "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
        "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
        "\n",
        "    # If cache_file is not None, try to read from it first\n",
        "    cache_data = None\n",
        "    if cache_file is not None:\n",
        "        try:\n",
        "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
        "                cache_data = pickle.load(f)\n",
        "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
        "        except:\n",
        "            pass  # unable to read from cache, but that's okay\n",
        "\n",
        "    # If cache is missing, then do the heavy lifting\n",
        "    if cache_data is None:\n",
        "        # Preprocess training and test data to obtain words for each review\n",
        "        #words_train = list(map(review_to_words, data_train))\n",
        "        #words_test = list(map(review_to_words, data_test))\n",
        "        words_train = [review_to_words(review) for review in data_train]\n",
        "        words_test = [review_to_words(review) for review in data_test]\n",
        "\n",
        "        # Write to cache file for future runs\n",
        "        if cache_file is not None:\n",
        "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
        "                              labels_train=labels_train, labels_test=labels_test)\n",
        "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
        "                pickle.dump(cache_data, f)\n",
        "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
        "    else:\n",
        "        # Unpack data loaded from cache file\n",
        "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
        "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
        "\n",
        "    return words_train, words_test, labels_train, labels_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymi-X0btGIsK",
        "outputId": "69878501-cdba-4de8-ac94-b754292e6ddb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Read preprocessed data from cache file: preprocessed_data.pkl\n"
          ]
        }
      ],
      "source": [
        "# Preprocess data\n",
        "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS1bFnjmGIsK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def build_dict(data, vocab_size = 5000):\n",
        "    \"\"\"Construct and return a dictionary mapping each of the most frequently appearing words to a unique integer.\"\"\"\n",
        "\n",
        "    # TODO: Determine how often each word appears in `data`. Note that `data` is a list of sentences and that a\n",
        "    #       sentence is a list of words.\n",
        "\n",
        "    word_count = {}\n",
        "\n",
        "    # A dict storing the words that appear in the reviews along with how often they occur\n",
        "\n",
        "    for review in data:\n",
        "        for word in review:\n",
        "            if word in word_count:\n",
        "                word_count[word] += 1\n",
        "            else:\n",
        "                word_count[word] = 1\n",
        "\n",
        "    # TODO: Sort the words found in `data` so that sorted_words[0] is the most frequently appearing word and\n",
        "    #       sorted_words[-1] is the least frequently appearing word.\n",
        "\n",
        "    word_count_sorted = sorted(word_count.items(), key=(lambda item: item[1]), reverse=True)\n",
        "    sorted_words = [item[0] for item in word_count_sorted]\n",
        "\n",
        "    word_dict = {} # This is what we are building, a dictionary that translates words into integers\n",
        "    for idx, word in enumerate(sorted_words[:vocab_size - 2]): # The -2 is so that we save room for the 'no word'\n",
        "        word_dict[word] = idx + 2                              # 'infrequent' labels\n",
        "\n",
        "    return word_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZDmgPEtGIsK",
        "outputId": "622fa8dc-488a-4a83-cbf0-ac2b30d87a27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'movi': 2,\n",
              " 'film': 3,\n",
              " 'one': 4,\n",
              " 'like': 5,\n",
              " 'time': 6,\n",
              " 'good': 7,\n",
              " 'make': 8,\n",
              " 'charact': 9,\n",
              " 'get': 10,\n",
              " 'see': 11,\n",
              " 'watch': 12,\n",
              " 'stori': 13,\n",
              " 'even': 14,\n",
              " 'would': 15,\n",
              " 'realli': 16,\n",
              " 'well': 17,\n",
              " 'scene': 18,\n",
              " 'look': 19,\n",
              " 'show': 20,\n",
              " 'much': 21,\n",
              " 'end': 22,\n",
              " 'peopl': 23,\n",
              " 'bad': 24,\n",
              " 'go': 25,\n",
              " 'great': 26,\n",
              " 'also': 27,\n",
              " 'first': 28,\n",
              " 'love': 29,\n",
              " 'think': 30,\n",
              " 'way': 31,\n",
              " 'act': 32,\n",
              " 'play': 33,\n",
              " 'made': 34,\n",
              " 'thing': 35,\n",
              " 'could': 36,\n",
              " 'know': 37,\n",
              " 'say': 38,\n",
              " 'seem': 39,\n",
              " 'work': 40,\n",
              " 'plot': 41,\n",
              " 'two': 42,\n",
              " 'actor': 43,\n",
              " 'year': 44,\n",
              " 'come': 45,\n",
              " 'mani': 46,\n",
              " 'seen': 47,\n",
              " 'take': 48,\n",
              " 'want': 49,\n",
              " 'life': 50,\n",
              " 'never': 51,\n",
              " 'littl': 52,\n",
              " 'best': 53,\n",
              " 'tri': 54,\n",
              " 'man': 55,\n",
              " 'ever': 56,\n",
              " 'give': 57,\n",
              " 'better': 58,\n",
              " 'still': 59,\n",
              " 'perform': 60,\n",
              " 'find': 61,\n",
              " 'feel': 62,\n",
              " 'part': 63,\n",
              " 'back': 64,\n",
              " 'use': 65,\n",
              " 'someth': 66,\n",
              " 'director': 67,\n",
              " 'actual': 68,\n",
              " 'interest': 69,\n",
              " 'lot': 70,\n",
              " 'real': 71,\n",
              " 'old': 72,\n",
              " 'cast': 73,\n",
              " 'though': 74,\n",
              " 'live': 75,\n",
              " 'star': 76,\n",
              " 'enjoy': 77,\n",
              " 'guy': 78,\n",
              " 'anoth': 79,\n",
              " 'new': 80,\n",
              " 'role': 81,\n",
              " 'noth': 82,\n",
              " '10': 83,\n",
              " 'funni': 84,\n",
              " 'music': 85,\n",
              " 'point': 86,\n",
              " 'start': 87,\n",
              " 'set': 88,\n",
              " 'girl': 89,\n",
              " 'origin': 90,\n",
              " 'day': 91,\n",
              " 'world': 92,\n",
              " 'everi': 93,\n",
              " 'believ': 94,\n",
              " 'turn': 95,\n",
              " 'quit': 96,\n",
              " 'us': 97,\n",
              " 'direct': 98,\n",
              " 'thought': 99,\n",
              " 'fact': 100,\n",
              " 'minut': 101,\n",
              " 'horror': 102,\n",
              " 'kill': 103,\n",
              " 'action': 104,\n",
              " 'comedi': 105,\n",
              " 'pretti': 106,\n",
              " 'young': 107,\n",
              " 'wonder': 108,\n",
              " 'happen': 109,\n",
              " 'around': 110,\n",
              " 'got': 111,\n",
              " 'effect': 112,\n",
              " 'right': 113,\n",
              " 'long': 114,\n",
              " 'howev': 115,\n",
              " 'big': 116,\n",
              " 'line': 117,\n",
              " 'famili': 118,\n",
              " 'enough': 119,\n",
              " 'seri': 120,\n",
              " 'may': 121,\n",
              " 'need': 122,\n",
              " 'fan': 123,\n",
              " 'bit': 124,\n",
              " 'script': 125,\n",
              " 'beauti': 126,\n",
              " 'person': 127,\n",
              " 'becom': 128,\n",
              " 'without': 129,\n",
              " 'must': 130,\n",
              " 'alway': 131,\n",
              " 'friend': 132,\n",
              " 'tell': 133,\n",
              " 'reason': 134,\n",
              " 'saw': 135,\n",
              " 'last': 136,\n",
              " 'final': 137,\n",
              " 'kid': 138,\n",
              " 'almost': 139,\n",
              " 'put': 140,\n",
              " 'least': 141,\n",
              " 'sure': 142,\n",
              " 'done': 143,\n",
              " 'whole': 144,\n",
              " 'place': 145,\n",
              " 'complet': 146,\n",
              " 'kind': 147,\n",
              " 'expect': 148,\n",
              " 'differ': 149,\n",
              " 'shot': 150,\n",
              " 'far': 151,\n",
              " 'mean': 152,\n",
              " 'anyth': 153,\n",
              " 'book': 154,\n",
              " 'laugh': 155,\n",
              " 'might': 156,\n",
              " 'name': 157,\n",
              " 'sinc': 158,\n",
              " 'begin': 159,\n",
              " '2': 160,\n",
              " 'probabl': 161,\n",
              " 'woman': 162,\n",
              " 'help': 163,\n",
              " 'entertain': 164,\n",
              " 'let': 165,\n",
              " 'screen': 166,\n",
              " 'call': 167,\n",
              " 'tv': 168,\n",
              " 'moment': 169,\n",
              " 'away': 170,\n",
              " 'read': 171,\n",
              " 'yet': 172,\n",
              " 'rather': 173,\n",
              " 'worst': 174,\n",
              " 'run': 175,\n",
              " 'fun': 176,\n",
              " 'lead': 177,\n",
              " 'hard': 178,\n",
              " 'audienc': 179,\n",
              " 'idea': 180,\n",
              " 'anyon': 181,\n",
              " 'episod': 182,\n",
              " 'american': 183,\n",
              " 'found': 184,\n",
              " 'appear': 185,\n",
              " 'bore': 186,\n",
              " 'especi': 187,\n",
              " 'although': 188,\n",
              " 'hope': 189,\n",
              " 'cours': 190,\n",
              " 'keep': 191,\n",
              " 'anim': 192,\n",
              " 'job': 193,\n",
              " 'goe': 194,\n",
              " 'move': 195,\n",
              " 'sens': 196,\n",
              " 'dvd': 197,\n",
              " 'version': 198,\n",
              " 'war': 199,\n",
              " 'money': 200,\n",
              " 'someon': 201,\n",
              " 'mind': 202,\n",
              " 'mayb': 203,\n",
              " 'problem': 204,\n",
              " 'true': 205,\n",
              " 'hous': 206,\n",
              " 'everyth': 207,\n",
              " 'nice': 208,\n",
              " 'second': 209,\n",
              " 'rate': 210,\n",
              " 'three': 211,\n",
              " 'night': 212,\n",
              " 'follow': 213,\n",
              " 'face': 214,\n",
              " 'recommend': 215,\n",
              " 'main': 216,\n",
              " 'product': 217,\n",
              " 'worth': 218,\n",
              " 'leav': 219,\n",
              " 'human': 220,\n",
              " 'special': 221,\n",
              " 'excel': 222,\n",
              " 'togeth': 223,\n",
              " 'wast': 224,\n",
              " 'everyon': 225,\n",
              " 'sound': 226,\n",
              " 'john': 227,\n",
              " 'hand': 228,\n",
              " '1': 229,\n",
              " 'father': 230,\n",
              " 'later': 231,\n",
              " 'eye': 232,\n",
              " 'said': 233,\n",
              " 'view': 234,\n",
              " 'instead': 235,\n",
              " 'review': 236,\n",
              " 'boy': 237,\n",
              " 'high': 238,\n",
              " 'hour': 239,\n",
              " 'miss': 240,\n",
              " 'classic': 241,\n",
              " 'talk': 242,\n",
              " 'wife': 243,\n",
              " 'understand': 244,\n",
              " 'left': 245,\n",
              " 'care': 246,\n",
              " 'black': 247,\n",
              " 'death': 248,\n",
              " 'open': 249,\n",
              " 'murder': 250,\n",
              " 'write': 251,\n",
              " 'half': 252,\n",
              " 'head': 253,\n",
              " 'rememb': 254,\n",
              " 'chang': 255,\n",
              " 'viewer': 256,\n",
              " 'fight': 257,\n",
              " 'gener': 258,\n",
              " 'surpris': 259,\n",
              " 'short': 260,\n",
              " 'includ': 261,\n",
              " 'die': 262,\n",
              " 'fall': 263,\n",
              " 'less': 264,\n",
              " 'els': 265,\n",
              " 'entir': 266,\n",
              " 'piec': 267,\n",
              " 'involv': 268,\n",
              " 'pictur': 269,\n",
              " 'simpli': 270,\n",
              " 'top': 271,\n",
              " 'home': 272,\n",
              " 'power': 273,\n",
              " 'total': 274,\n",
              " 'usual': 275,\n",
              " 'budget': 276,\n",
              " 'attempt': 277,\n",
              " 'suppos': 278,\n",
              " 'releas': 279,\n",
              " 'hollywood': 280,\n",
              " 'terribl': 281,\n",
              " 'song': 282,\n",
              " 'men': 283,\n",
              " 'possibl': 284,\n",
              " 'featur': 285,\n",
              " 'portray': 286,\n",
              " 'disappoint': 287,\n",
              " '3': 288,\n",
              " 'poor': 289,\n",
              " 'coupl': 290,\n",
              " 'camera': 291,\n",
              " 'stupid': 292,\n",
              " 'dead': 293,\n",
              " 'wrong': 294,\n",
              " 'low': 295,\n",
              " 'produc': 296,\n",
              " 'video': 297,\n",
              " 'either': 298,\n",
              " 'aw': 299,\n",
              " 'definit': 300,\n",
              " 'except': 301,\n",
              " 'rest': 302,\n",
              " 'given': 303,\n",
              " 'absolut': 304,\n",
              " 'women': 305,\n",
              " 'lack': 306,\n",
              " 'word': 307,\n",
              " 'writer': 308,\n",
              " 'titl': 309,\n",
              " 'talent': 310,\n",
              " 'decid': 311,\n",
              " 'full': 312,\n",
              " 'perfect': 313,\n",
              " 'along': 314,\n",
              " 'style': 315,\n",
              " 'close': 316,\n",
              " 'truli': 317,\n",
              " 'school': 318,\n",
              " 'emot': 319,\n",
              " 'save': 320,\n",
              " 'age': 321,\n",
              " 'sex': 322,\n",
              " 'next': 323,\n",
              " 'bring': 324,\n",
              " 'mr': 325,\n",
              " 'case': 326,\n",
              " 'killer': 327,\n",
              " 'heart': 328,\n",
              " 'comment': 329,\n",
              " 'sort': 330,\n",
              " 'creat': 331,\n",
              " 'perhap': 332,\n",
              " 'came': 333,\n",
              " 'brother': 334,\n",
              " 'sever': 335,\n",
              " 'joke': 336,\n",
              " 'art': 337,\n",
              " 'dialogu': 338,\n",
              " 'game': 339,\n",
              " 'small': 340,\n",
              " 'base': 341,\n",
              " 'flick': 342,\n",
              " 'written': 343,\n",
              " 'sequenc': 344,\n",
              " 'meet': 345,\n",
              " 'earli': 346,\n",
              " 'often': 347,\n",
              " 'other': 348,\n",
              " 'mother': 349,\n",
              " 'develop': 350,\n",
              " 'humor': 351,\n",
              " 'actress': 352,\n",
              " 'consid': 353,\n",
              " 'dark': 354,\n",
              " 'guess': 355,\n",
              " 'amaz': 356,\n",
              " 'unfortun': 357,\n",
              " 'lost': 358,\n",
              " 'light': 359,\n",
              " 'exampl': 360,\n",
              " 'cinema': 361,\n",
              " 'drama': 362,\n",
              " 'white': 363,\n",
              " 'ye': 364,\n",
              " 'experi': 365,\n",
              " 'imagin': 366,\n",
              " 'mention': 367,\n",
              " 'stop': 368,\n",
              " 'natur': 369,\n",
              " 'forc': 370,\n",
              " 'manag': 371,\n",
              " 'felt': 372,\n",
              " 'cut': 373,\n",
              " 'present': 374,\n",
              " 'children': 375,\n",
              " 'fail': 376,\n",
              " 'son': 377,\n",
              " 'qualiti': 378,\n",
              " 'car': 379,\n",
              " 'support': 380,\n",
              " 'ask': 381,\n",
              " 'hit': 382,\n",
              " 'side': 383,\n",
              " 'voic': 384,\n",
              " 'extrem': 385,\n",
              " 'impress': 386,\n",
              " 'evil': 387,\n",
              " 'wors': 388,\n",
              " 'stand': 389,\n",
              " 'went': 390,\n",
              " 'certainli': 391,\n",
              " 'basic': 392,\n",
              " 'oh': 393,\n",
              " 'overal': 394,\n",
              " 'favorit': 395,\n",
              " 'horribl': 396,\n",
              " 'mysteri': 397,\n",
              " 'number': 398,\n",
              " 'type': 399,\n",
              " 'danc': 400,\n",
              " 'wait': 401,\n",
              " 'hero': 402,\n",
              " '5': 403,\n",
              " 'alreadi': 404,\n",
              " 'learn': 405,\n",
              " 'matter': 406,\n",
              " '4': 407,\n",
              " 'michael': 408,\n",
              " 'genr': 409,\n",
              " 'fine': 410,\n",
              " 'despit': 411,\n",
              " 'throughout': 412,\n",
              " 'walk': 413,\n",
              " 'success': 414,\n",
              " 'histori': 415,\n",
              " 'question': 416,\n",
              " 'zombi': 417,\n",
              " 'town': 418,\n",
              " 'realiz': 419,\n",
              " 'relationship': 420,\n",
              " 'past': 421,\n",
              " 'child': 422,\n",
              " 'daughter': 423,\n",
              " 'late': 424,\n",
              " 'b': 425,\n",
              " 'wish': 426,\n",
              " 'credit': 427,\n",
              " 'hate': 428,\n",
              " 'event': 429,\n",
              " 'theme': 430,\n",
              " 'touch': 431,\n",
              " 'citi': 432,\n",
              " 'today': 433,\n",
              " 'sometim': 434,\n",
              " 'behind': 435,\n",
              " 'god': 436,\n",
              " 'twist': 437,\n",
              " 'sit': 438,\n",
              " 'annoy': 439,\n",
              " 'stay': 440,\n",
              " 'deal': 441,\n",
              " 'abl': 442,\n",
              " 'rent': 443,\n",
              " 'pleas': 444,\n",
              " 'edit': 445,\n",
              " 'blood': 446,\n",
              " 'deserv': 447,\n",
              " 'anyway': 448,\n",
              " 'comic': 449,\n",
              " 'appar': 450,\n",
              " 'soon': 451,\n",
              " 'gave': 452,\n",
              " 'etc': 453,\n",
              " 'level': 454,\n",
              " 'slow': 455,\n",
              " 'chanc': 456,\n",
              " 'score': 457,\n",
              " 'bodi': 458,\n",
              " 'brilliant': 459,\n",
              " 'incred': 460,\n",
              " 'figur': 461,\n",
              " 'situat': 462,\n",
              " 'major': 463,\n",
              " 'self': 464,\n",
              " 'stuff': 465,\n",
              " 'decent': 466,\n",
              " 'element': 467,\n",
              " 'return': 468,\n",
              " 'dream': 469,\n",
              " 'obvious': 470,\n",
              " 'order': 471,\n",
              " 'continu': 472,\n",
              " 'pace': 473,\n",
              " 'ridicul': 474,\n",
              " 'happi': 475,\n",
              " 'add': 476,\n",
              " 'highli': 477,\n",
              " 'group': 478,\n",
              " 'thank': 479,\n",
              " 'ladi': 480,\n",
              " 'novel': 481,\n",
              " 'speak': 482,\n",
              " 'pain': 483,\n",
              " 'career': 484,\n",
              " 'shoot': 485,\n",
              " 'strang': 486,\n",
              " 'heard': 487,\n",
              " 'sad': 488,\n",
              " 'husband': 489,\n",
              " 'polic': 490,\n",
              " 'import': 491,\n",
              " 'break': 492,\n",
              " 'took': 493,\n",
              " 'strong': 494,\n",
              " 'cannot': 495,\n",
              " 'predict': 496,\n",
              " 'robert': 497,\n",
              " 'violenc': 498,\n",
              " 'hilari': 499,\n",
              " 'recent': 500,\n",
              " 'countri': 501,\n",
              " 'known': 502,\n",
              " 'particularli': 503,\n",
              " 'pick': 504,\n",
              " 'documentari': 505,\n",
              " 'season': 506,\n",
              " 'critic': 507,\n",
              " 'jame': 508,\n",
              " 'compar': 509,\n",
              " 'alon': 510,\n",
              " 'obviou': 511,\n",
              " 'told': 512,\n",
              " 'state': 513,\n",
              " 'visual': 514,\n",
              " 'rock': 515,\n",
              " 'offer': 516,\n",
              " 'theater': 517,\n",
              " 'exist': 518,\n",
              " 'opinion': 519,\n",
              " 'gore': 520,\n",
              " 'crap': 521,\n",
              " 'hold': 522,\n",
              " 'result': 523,\n",
              " 'room': 524,\n",
              " 'hear': 525,\n",
              " 'realiti': 526,\n",
              " 'clich': 527,\n",
              " 'effort': 528,\n",
              " 'thriller': 529,\n",
              " 'caus': 530,\n",
              " 'sequel': 531,\n",
              " 'explain': 532,\n",
              " 'serious': 533,\n",
              " 'king': 534,\n",
              " 'local': 535,\n",
              " 'ago': 536,\n",
              " 'none': 537,\n",
              " 'hell': 538,\n",
              " 'note': 539,\n",
              " 'allow': 540,\n",
              " 'sister': 541,\n",
              " 'david': 542,\n",
              " 'simpl': 543,\n",
              " 'femal': 544,\n",
              " 'deliv': 545,\n",
              " 'ok': 546,\n",
              " 'class': 547,\n",
              " 'convinc': 548,\n",
              " 'check': 549,\n",
              " 'suspens': 550,\n",
              " 'win': 551,\n",
              " 'oscar': 552,\n",
              " 'buy': 553,\n",
              " 'huge': 554,\n",
              " 'valu': 555,\n",
              " 'sexual': 556,\n",
              " 'scari': 557,\n",
              " 'cool': 558,\n",
              " 'similar': 559,\n",
              " 'excit': 560,\n",
              " 'exactli': 561,\n",
              " 'apart': 562,\n",
              " 'provid': 563,\n",
              " 'avoid': 564,\n",
              " 'shown': 565,\n",
              " 'seriou': 566,\n",
              " 'english': 567,\n",
              " 'whose': 568,\n",
              " 'taken': 569,\n",
              " 'cinematographi': 570,\n",
              " 'shock': 571,\n",
              " 'polit': 572,\n",
              " 'spoiler': 573,\n",
              " 'offic': 574,\n",
              " 'across': 575,\n",
              " 'middl': 576,\n",
              " 'street': 577,\n",
              " 'pass': 578,\n",
              " 'messag': 579,\n",
              " 'charm': 580,\n",
              " 'somewhat': 581,\n",
              " 'silli': 582,\n",
              " 'modern': 583,\n",
              " 'filmmak': 584,\n",
              " 'confus': 585,\n",
              " 'form': 586,\n",
              " 'tale': 587,\n",
              " 'singl': 588,\n",
              " 'jack': 589,\n",
              " 'mostli': 590,\n",
              " 'carri': 591,\n",
              " 'william': 592,\n",
              " 'attent': 593,\n",
              " 'sing': 594,\n",
              " 'subject': 595,\n",
              " 'five': 596,\n",
              " 'prove': 597,\n",
              " 'richard': 598,\n",
              " 'team': 599,\n",
              " 'stage': 600,\n",
              " 'unlik': 601,\n",
              " 'cop': 602,\n",
              " 'georg': 603,\n",
              " 'monster': 604,\n",
              " 'televis': 605,\n",
              " 'earth': 606,\n",
              " 'cover': 607,\n",
              " 'villain': 608,\n",
              " 'pay': 609,\n",
              " 'marri': 610,\n",
              " 'toward': 611,\n",
              " 'build': 612,\n",
              " 'pull': 613,\n",
              " 'parent': 614,\n",
              " 'due': 615,\n",
              " 'fill': 616,\n",
              " 'respect': 617,\n",
              " 'four': 618,\n",
              " 'dialog': 619,\n",
              " 'remind': 620,\n",
              " 'futur': 621,\n",
              " 'typic': 622,\n",
              " 'weak': 623,\n",
              " '7': 624,\n",
              " 'cheap': 625,\n",
              " 'intellig': 626,\n",
              " 'atmospher': 627,\n",
              " 'british': 628,\n",
              " 'clearli': 629,\n",
              " '80': 630,\n",
              " 'dog': 631,\n",
              " 'non': 632,\n",
              " 'paul': 633,\n",
              " 'knew': 634,\n",
              " 'fast': 635,\n",
              " 'artist': 636,\n",
              " '8': 637,\n",
              " 'crime': 638,\n",
              " 'easili': 639,\n",
              " 'escap': 640,\n",
              " 'doubt': 641,\n",
              " 'adult': 642,\n",
              " 'detail': 643,\n",
              " 'date': 644,\n",
              " 'member': 645,\n",
              " 'romant': 646,\n",
              " 'fire': 647,\n",
              " 'gun': 648,\n",
              " 'drive': 649,\n",
              " 'straight': 650,\n",
              " 'beyond': 651,\n",
              " 'fit': 652,\n",
              " 'attack': 653,\n",
              " 'imag': 654,\n",
              " 'upon': 655,\n",
              " 'posit': 656,\n",
              " 'whether': 657,\n",
              " 'fantast': 658,\n",
              " 'peter': 659,\n",
              " 'captur': 660,\n",
              " 'appreci': 661,\n",
              " 'aspect': 662,\n",
              " 'ten': 663,\n",
              " 'plan': 664,\n",
              " 'discov': 665,\n",
              " 'remain': 666,\n",
              " 'period': 667,\n",
              " 'near': 668,\n",
              " 'realist': 669,\n",
              " 'air': 670,\n",
              " 'mark': 671,\n",
              " 'red': 672,\n",
              " 'dull': 673,\n",
              " 'adapt': 674,\n",
              " 'within': 675,\n",
              " 'spend': 676,\n",
              " 'lose': 677,\n",
              " 'color': 678,\n",
              " 'materi': 679,\n",
              " 'chase': 680,\n",
              " 'mari': 681,\n",
              " 'storylin': 682,\n",
              " 'forget': 683,\n",
              " 'bunch': 684,\n",
              " 'clear': 685,\n",
              " 'lee': 686,\n",
              " 'victim': 687,\n",
              " 'nearli': 688,\n",
              " 'box': 689,\n",
              " 'york': 690,\n",
              " 'inspir': 691,\n",
              " 'match': 692,\n",
              " 'finish': 693,\n",
              " 'mess': 694,\n",
              " 'standard': 695,\n",
              " 'easi': 696,\n",
              " 'truth': 697,\n",
              " 'suffer': 698,\n",
              " 'busi': 699,\n",
              " 'dramat': 700,\n",
              " 'space': 701,\n",
              " 'bill': 702,\n",
              " 'western': 703,\n",
              " 'e': 704,\n",
              " 'list': 705,\n",
              " 'battl': 706,\n",
              " 'notic': 707,\n",
              " 'de': 708,\n",
              " 'french': 709,\n",
              " 'ad': 710,\n",
              " '9': 711,\n",
              " 'tom': 712,\n",
              " 'larg': 713,\n",
              " 'among': 714,\n",
              " 'eventu': 715,\n",
              " 'train': 716,\n",
              " 'accept': 717,\n",
              " 'agre': 718,\n",
              " 'spirit': 719,\n",
              " 'soundtrack': 720,\n",
              " 'third': 721,\n",
              " 'teenag': 722,\n",
              " 'soldier': 723,\n",
              " 'adventur': 724,\n",
              " 'suggest': 725,\n",
              " 'drug': 726,\n",
              " 'sorri': 727,\n",
              " 'famou': 728,\n",
              " 'cri': 729,\n",
              " 'normal': 730,\n",
              " 'babi': 731,\n",
              " 'ultim': 732,\n",
              " 'troubl': 733,\n",
              " 'contain': 734,\n",
              " 'certain': 735,\n",
              " 'cultur': 736,\n",
              " 'romanc': 737,\n",
              " 'rare': 738,\n",
              " 'lame': 739,\n",
              " 'somehow': 740,\n",
              " 'mix': 741,\n",
              " 'disney': 742,\n",
              " 'gone': 743,\n",
              " 'cartoon': 744,\n",
              " 'student': 745,\n",
              " 'fear': 746,\n",
              " 'reveal': 747,\n",
              " 'suck': 748,\n",
              " 'kept': 749,\n",
              " 'attract': 750,\n",
              " 'appeal': 751,\n",
              " 'premis': 752,\n",
              " 'greatest': 753,\n",
              " 'design': 754,\n",
              " 'secret': 755,\n",
              " 'shame': 756,\n",
              " 'throw': 757,\n",
              " 'scare': 758,\n",
              " 'copi': 759,\n",
              " 'wit': 760,\n",
              " 'america': 761,\n",
              " 'admit': 762,\n",
              " 'particular': 763,\n",
              " 'brought': 764,\n",
              " 'relat': 765,\n",
              " 'screenplay': 766,\n",
              " 'whatev': 767,\n",
              " 'pure': 768,\n",
              " '70': 769,\n",
              " 'averag': 770,\n",
              " 'harri': 771,\n",
              " 'master': 772,\n",
              " 'describ': 773,\n",
              " 'treat': 774,\n",
              " 'male': 775,\n",
              " '20': 776,\n",
              " 'fantasi': 777,\n",
              " 'issu': 778,\n",
              " 'warn': 779,\n",
              " 'inde': 780,\n",
              " 'forward': 781,\n",
              " 'background': 782,\n",
              " 'project': 783,\n",
              " 'free': 784,\n",
              " 'japanes': 785,\n",
              " 'memor': 786,\n",
              " 'poorli': 787,\n",
              " 'award': 788,\n",
              " 'locat': 789,\n",
              " 'potenti': 790,\n",
              " 'amus': 791,\n",
              " 'struggl': 792,\n",
              " 'weird': 793,\n",
              " 'magic': 794,\n",
              " 'societi': 795,\n",
              " 'okay': 796,\n",
              " 'imdb': 797,\n",
              " 'accent': 798,\n",
              " 'doctor': 799,\n",
              " 'hot': 800,\n",
              " 'water': 801,\n",
              " 'alien': 802,\n",
              " '30': 803,\n",
              " 'dr': 804,\n",
              " 'express': 805,\n",
              " 'odd': 806,\n",
              " 'crazi': 807,\n",
              " 'choic': 808,\n",
              " 'studio': 809,\n",
              " 'fiction': 810,\n",
              " 'becam': 811,\n",
              " 'control': 812,\n",
              " 'masterpiec': 813,\n",
              " 'difficult': 814,\n",
              " 'fli': 815,\n",
              " 'joe': 816,\n",
              " 'scream': 817,\n",
              " 'costum': 818,\n",
              " 'lover': 819,\n",
              " 'uniqu': 820,\n",
              " 'refer': 821,\n",
              " 'remak': 822,\n",
              " 'girlfriend': 823,\n",
              " 'vampir': 824,\n",
              " 'prison': 825,\n",
              " 'execut': 826,\n",
              " 'wear': 827,\n",
              " 'jump': 828,\n",
              " 'unless': 829,\n",
              " 'wood': 830,\n",
              " 'creepi': 831,\n",
              " 'cheesi': 832,\n",
              " 'superb': 833,\n",
              " 'otherwis': 834,\n",
              " 'parti': 835,\n",
              " 'ghost': 836,\n",
              " 'roll': 837,\n",
              " 'public': 838,\n",
              " 'mad': 839,\n",
              " 'depict': 840,\n",
              " 'week': 841,\n",
              " 'earlier': 842,\n",
              " 'jane': 843,\n",
              " 'moral': 844,\n",
              " 'badli': 845,\n",
              " 'fi': 846,\n",
              " 'dumb': 847,\n",
              " 'grow': 848,\n",
              " 'flaw': 849,\n",
              " 'sci': 850,\n",
              " 'deep': 851,\n",
              " 'maker': 852,\n",
              " 'cat': 853,\n",
              " 'connect': 854,\n",
              " 'footag': 855,\n",
              " 'older': 856,\n",
              " 'bother': 857,\n",
              " 'plenti': 858,\n",
              " 'outsid': 859,\n",
              " 'stick': 860,\n",
              " 'gay': 861,\n",
              " 'catch': 862,\n",
              " 'co': 863,\n",
              " 'plu': 864,\n",
              " 'popular': 865,\n",
              " 'equal': 866,\n",
              " 'social': 867,\n",
              " 'quickli': 868,\n",
              " 'disturb': 869,\n",
              " 'perfectli': 870,\n",
              " 'dress': 871,\n",
              " '90': 872,\n",
              " 'era': 873,\n",
              " 'mistak': 874,\n",
              " 'lie': 875,\n",
              " 'previou': 876,\n",
              " 'ride': 877,\n",
              " 'combin': 878,\n",
              " 'band': 879,\n",
              " 'concept': 880,\n",
              " 'answer': 881,\n",
              " 'rich': 882,\n",
              " 'surviv': 883,\n",
              " 'front': 884,\n",
              " 'christma': 885,\n",
              " 'sweet': 886,\n",
              " 'insid': 887,\n",
              " 'concern': 888,\n",
              " 'bare': 889,\n",
              " 'eat': 890,\n",
              " 'ben': 891,\n",
              " 'listen': 892,\n",
              " 'beat': 893,\n",
              " 'c': 894,\n",
              " 'serv': 895,\n",
              " 'term': 896,\n",
              " 'german': 897,\n",
              " 'meant': 898,\n",
              " 'la': 899,\n",
              " 'hardli': 900,\n",
              " 'stereotyp': 901,\n",
              " 'innoc': 902,\n",
              " 'law': 903,\n",
              " 'desper': 904,\n",
              " 'promis': 905,\n",
              " 'memori': 906,\n",
              " 'cute': 907,\n",
              " 'intent': 908,\n",
              " 'inform': 909,\n",
              " 'steal': 910,\n",
              " 'variou': 911,\n",
              " 'brain': 912,\n",
              " 'post': 913,\n",
              " 'tone': 914,\n",
              " 'island': 915,\n",
              " 'amount': 916,\n",
              " 'compani': 917,\n",
              " 'nuditi': 918,\n",
              " 'track': 919,\n",
              " 'claim': 920,\n",
              " 'store': 921,\n",
              " '50': 922,\n",
              " 'hair': 923,\n",
              " 'flat': 924,\n",
              " 'univers': 925,\n",
              " 'land': 926,\n",
              " 'scott': 927,\n",
              " 'fairli': 928,\n",
              " 'danger': 929,\n",
              " 'kick': 930,\n",
              " 'player': 931,\n",
              " 'step': 932,\n",
              " 'plain': 933,\n",
              " 'crew': 934,\n",
              " 'toni': 935,\n",
              " 'share': 936,\n",
              " 'centuri': 937,\n",
              " 'tast': 938,\n",
              " 'engag': 939,\n",
              " 'achiev': 940,\n",
              " 'cold': 941,\n",
              " 'travel': 942,\n",
              " 'record': 943,\n",
              " 'suit': 944,\n",
              " 'rip': 945,\n",
              " 'sadli': 946,\n",
              " 'manner': 947,\n",
              " 'wrote': 948,\n",
              " 'spot': 949,\n",
              " 'tension': 950,\n",
              " 'intens': 951,\n",
              " 'fascin': 952,\n",
              " 'familiar': 953,\n",
              " 'remark': 954,\n",
              " 'burn': 955,\n",
              " 'depth': 956,\n",
              " 'destroy': 957,\n",
              " 'histor': 958,\n",
              " 'sleep': 959,\n",
              " 'purpos': 960,\n",
              " 'languag': 961,\n",
              " 'ruin': 962,\n",
              " 'ignor': 963,\n",
              " 'delight': 964,\n",
              " 'unbeliev': 965,\n",
              " 'italian': 966,\n",
              " 'abil': 967,\n",
              " 'soul': 968,\n",
              " 'collect': 969,\n",
              " 'detect': 970,\n",
              " 'clever': 971,\n",
              " 'violent': 972,\n",
              " 'rape': 973,\n",
              " 'reach': 974,\n",
              " 'door': 975,\n",
              " 'trash': 976,\n",
              " 'scienc': 977,\n",
              " 'liter': 978,\n",
              " 'commun': 979,\n",
              " 'caught': 980,\n",
              " 'reveng': 981,\n",
              " 'creatur': 982,\n",
              " 'approach': 983,\n",
              " 'trip': 984,\n",
              " 'intrigu': 985,\n",
              " 'fashion': 986,\n",
              " 'introduc': 987,\n",
              " 'paint': 988,\n",
              " 'skill': 989,\n",
              " 'complex': 990,\n",
              " 'channel': 991,\n",
              " 'camp': 992,\n",
              " 'christian': 993,\n",
              " 'extra': 994,\n",
              " 'hole': 995,\n",
              " 'mental': 996,\n",
              " 'ann': 997,\n",
              " 'limit': 998,\n",
              " 'immedi': 999,\n",
              " 'mere': 1000,\n",
              " 'comput': 1001,\n",
              " ...}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_dict = build_dict(train_X)\n",
        "word_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXcp8XFiGIsL",
        "outputId": "e993c6bd-7032-467f-d5f8-7e0ca47e6d6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['movi', 'film', 'one', 'like', 'time']"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(word_dict.keys())[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSUheGkAGIsL"
      },
      "source": [
        "### Save `word_dict`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hw30bq1tGIsM"
      },
      "outputs": [],
      "source": [
        "data_dir = '../data/pytorch' # The folder we will use for storing data\n",
        "if not os.path.exists(data_dir): # Make sure that the folder exists\n",
        "    os.makedirs(data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "az8BUa9EGIsM"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(data_dir, 'word_dict.pkl'), \"wb\") as f:\n",
        "    pickle.dump(word_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KdkJUwHGIsM"
      },
      "source": [
        "### Transform the reviews\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUnX9HwCGIsM"
      },
      "outputs": [],
      "source": [
        "def convert_and_pad(word_dict, sentence, pad=500):\n",
        "    NOWORD = 0 # We will use 0 to represent the 'no word' category\n",
        "    INFREQ = 1 # and we use 1 to represent the infrequent words, i.e., words not appearing in word_dict\n",
        "\n",
        "    working_sentence = [NOWORD] * pad\n",
        "\n",
        "    for word_index, word in enumerate(sentence[:pad]):\n",
        "        if word in word_dict:\n",
        "            working_sentence[word_index] = word_dict[word]\n",
        "        else:\n",
        "            working_sentence[word_index] = INFREQ\n",
        "\n",
        "    return working_sentence, min(len(sentence), pad)\n",
        "\n",
        "def convert_and_pad_data(word_dict, data, pad=500):\n",
        "    result = []\n",
        "    lengths = []\n",
        "\n",
        "    for sentence in data:\n",
        "        converted, leng = convert_and_pad(word_dict, sentence, pad)\n",
        "        result.append(converted)\n",
        "        lengths.append(leng)\n",
        "\n",
        "    return np.array(result), np.array(lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKMkHgH5GIsM"
      },
      "outputs": [],
      "source": [
        "train_X, train_X_len = convert_and_pad_data(word_dict, train_X)\n",
        "test_X, test_X_len = convert_and_pad_data(word_dict, test_X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cByfxvfCGIsN",
        "outputId": "40d1d8d5-2fe5-476c-cc78-b28990eeb620"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2029,  655, 3423,  221,  424,    4,  212,  304,  569,  750,  628,\n",
              "        826,    1,   51,  155,  178, 1476,  415,  146, 2331,   86, 1448,\n",
              "          1,   34,  155,   21,    1,  349,  959,   22,  206,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use this cell to examine one of the processed reviews to make sure everything is working as intended.\n",
        "\n",
        "\n",
        "train_X[100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_4mcIvmGIsN"
      },
      "source": [
        "## Step 3: Upload the data to S3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZGEgIFaGIsN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.concat([pd.DataFrame(train_y), pd.DataFrame(train_X_len), pd.DataFrame(train_X)], axis=1) \\\n",
        "        .to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOUwtUh6GIsT"
      },
      "source": [
        "### Uploading the training data\n",
        "\n",
        "\n",
        "Next, we need to upload the training data to the SageMaker default S3 bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJQW0F7pGIsT"
      },
      "outputs": [],
      "source": [
        "import sagemaker\n",
        "\n",
        "sagemaker_session = sagemaker.Session()\n",
        "\n",
        "bucket = sagemaker_session.default_bucket()\n",
        "prefix = 'sagemaker/sentiment_rnn'\n",
        "\n",
        "role = sagemaker.get_execution_role()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaRavVprGIsT"
      },
      "outputs": [],
      "source": [
        "input_data = sagemaker_session.upload_data(path=data_dir, bucket=bucket, key_prefix=prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47l2WkV9GIsU"
      },
      "source": [
        "## Step 4: Build and Train the PyTorch Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGIx6A_mGIsU",
        "outputId": "98e04e05-9c91-4dca-9168-d1f892b4393b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
            "\r\n",
            "\u001b[34mclass\u001b[39;49;00m \u001b[04m\u001b[32mLSTMClassifier\u001b[39;49;00m(nn.Module):\r\n",
            "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m    This is the simple RNN model we will be using to perform Sentiment Analysis.\u001b[39;49;00m\r\n",
            "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32m__init__\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, embedding_dim, hidden_dim, vocab_size):\r\n",
            "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m        Initialize the model by settingg up the various layers.\u001b[39;49;00m\r\n",
            "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
            "        \u001b[36msuper\u001b[39;49;00m(LSTMClassifier, \u001b[36mself\u001b[39;49;00m).\u001b[32m__init__\u001b[39;49;00m()\r\n",
            "\r\n",
            "        \u001b[36mself\u001b[39;49;00m.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=\u001b[34m0\u001b[39;49;00m)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.lstm = nn.LSTM(embedding_dim, hidden_dim)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.dense = nn.Linear(in_features=hidden_dim, out_features=\u001b[34m1\u001b[39;49;00m)\r\n",
            "        \u001b[36mself\u001b[39;49;00m.sig = nn.Sigmoid()\r\n",
            "        \r\n",
            "        \u001b[36mself\u001b[39;49;00m.word_dict = \u001b[34mNone\u001b[39;49;00m\r\n",
            "\r\n",
            "    \u001b[34mdef\u001b[39;49;00m \u001b[32mforward\u001b[39;49;00m(\u001b[36mself\u001b[39;49;00m, x):\r\n",
            "        \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
            "\u001b[33m        Perform a forward pass of our model on some input.\u001b[39;49;00m\r\n",
            "\u001b[33m        \"\"\"\u001b[39;49;00m\r\n",
            "        x = x.t()\r\n",
            "        lengths = x[\u001b[34m0\u001b[39;49;00m,:]\r\n",
            "        reviews = x[\u001b[34m1\u001b[39;49;00m:,:]\r\n",
            "        embeds = \u001b[36mself\u001b[39;49;00m.embedding(reviews)\r\n",
            "        lstm_out, _ = \u001b[36mself\u001b[39;49;00m.lstm(embeds)\r\n",
            "        out = \u001b[36mself\u001b[39;49;00m.dense(lstm_out)\r\n",
            "        out = out[lengths - \u001b[34m1\u001b[39;49;00m, \u001b[36mrange\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(lengths))]\r\n",
            "        \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.sig(out.squeeze())\r\n"
          ]
        }
      ],
      "source": [
        "!pygmentize train/model.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mD9dF9BGIsV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "\n",
        "# Read in only the first 250 rows\n",
        "train_sample = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None, names=None, nrows=250)\n",
        "\n",
        "# Turn the input pandas dataframe into tensors\n",
        "train_sample_y = torch.from_numpy(train_sample[[0]].values).float().squeeze()\n",
        "train_sample_X = torch.from_numpy(train_sample.drop([0], axis=1).values).long()\n",
        "\n",
        "# Build the dataset\n",
        "train_sample_ds = torch.utils.data.TensorDataset(train_sample_X, train_sample_y)\n",
        "# Build the dataloader\n",
        "train_sample_dl = torch.utils.data.DataLoader(train_sample_ds, batch_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3ZR27XNGIsV"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, epochs, optimizer, loss_fn, device):\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for batch in train_loader:\n",
        "            batch_X, batch_y = batch\n",
        "\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "\n",
        "\n",
        "     # TODO: Complete this train method to train the model provided.\n",
        "\n",
        "     ## The task  : here I copy and paste  the train method  provided in my Trai.py\n",
        "            optimizer.zero_grad()\n",
        "            out = model.forward(batch_X) ####\n",
        "            loss = loss_fn(out, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.data.item()\n",
        "        print(\"Epoch: {}, BCELoss: {}\".format(epoch, total_loss / len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NlnlPlfGIsV",
        "outputId": "6b556b04-548c-49c5-fb7a-cd04cd372973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, BCELoss: 0.6883740901947022\n",
            "Epoch: 2, BCELoss: 0.67933509349823\n",
            "Epoch: 3, BCELoss: 0.6712734222412109\n",
            "Epoch: 4, BCELoss: 0.6618405222892761\n",
            "Epoch: 5, BCELoss: 0.6493316411972045\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from train.model import LSTMClassifier\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMClassifier(32, 100, 5000).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = torch.nn.BCELoss()\n",
        "\n",
        "train(model, train_sample_dl, 5, optimizer, loss_fn, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9wYUJJoGIsW"
      },
      "outputs": [],
      "source": [
        "from sagemaker.pytorch import PyTorch\n",
        "\n",
        "estimator = PyTorch(entry_point=\"train.py\",\n",
        "                    source_dir=\"train\",\n",
        "                    role=role,\n",
        "                    framework_version='0.4.0',\n",
        "                    train_instance_count=1,\n",
        "                    train_instance_type='ml.p2.xlarge',\n",
        "                    hyperparameters={\n",
        "                        'epochs': 10,\n",
        "                        'hidden_dim': 200,\n",
        "                    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGlx9uzeGIsW",
        "outputId": "810dc722-ab18-4963-d996-a7f84e846b2a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
            "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
            "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020-09-18 03:54:10 Starting - Starting the training job...\n",
            "2020-09-18 03:54:13 Starting - Launching requested ML instances......\n",
            "2020-09-18 03:55:26 Starting - Preparing the instances for training.........\n",
            "2020-09-18 03:56:50 Downloading - Downloading input data\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
            "\u001b[34mbash: no job control in this shell\u001b[0m\n",
            "\u001b[34m2020-09-18 03:57:57,379 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
            "\u001b[34m2020-09-18 03:57:57,423 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
            "\u001b[34m2020-09-18 03:57:57,428 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
            "\u001b[34m2020-09-18 03:57:57,677 sagemaker-containers INFO     Module train does not provide a setup.py. \u001b[0m\n",
            "\u001b[34mGenerating setup.py\u001b[0m\n",
            "\u001b[34m2020-09-18 03:57:57,677 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
            "\u001b[34m2020-09-18 03:57:57,677 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
            "\u001b[34m2020-09-18 03:57:57,677 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
            "\u001b[34m/usr/bin/python -m pip install -U . -r requirements.txt\u001b[0m\n",
            "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
            "\u001b[34mCollecting pandas (from -r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/74/24/0cdbf8907e1e3bc5a8da03345c23cbed7044330bb8f73bb12e711a640a00/pandas-0.24.2-cp35-cp35m-manylinux1_x86_64.whl (10.0MB)\u001b[0m\n",
            "\u001b[34mCollecting numpy (from -r requirements.txt (line 2))\u001b[0m\n",
            "\u001b[34m  Downloading https://files.pythonhosted.org/packages/b5/36/88723426b4ff576809fec7d73594fe17a35c27f8d01f93637637a29ae25b/numpy-1.18.5-cp35-cp35m-manylinux1_x86_64.whl (19.9MB)\u001b[0m\n",
            "\u001b[34mCollecting nltk (from -r requirements.txt (line 3))\u001b[0m\n",
            "\u001b[34m  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\u001b[0m\n",
            "\u001b[34mCollecting beautifulsoup4 (from -r requirements.txt (line 4))\n",
            "  Downloading https://files.pythonhosted.org/packages/66/25/ff030e2437265616a1e9b25ccc864e0371a0bc3adb7c5a404fd661c6f4f6/beautifulsoup4-4.9.1-py3-none-any.whl (115kB)\u001b[0m\n",
            "\u001b[34mCollecting html5lib (from -r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/dd/a834df6482147d48e225a49515aabc28974ad5a4ca3215c18a882565b028/html5lib-1.1-py2.py3-none-any.whl (112kB)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil>=2.5.0 in /usr/local/lib/python3.5/dist-packages (from pandas->-r requirements.txt (line 1)) (2.7.5)\u001b[0m\n",
            "\u001b[34mCollecting pytz>=2011k (from pandas->-r requirements.txt (line 1))\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl (510kB)\u001b[0m\n",
            "\u001b[34mRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.5/dist-packages (from nltk->-r requirements.txt (line 3)) (7.0)\u001b[0m\n",
            "\u001b[34mCollecting joblib (from nltk->-r requirements.txt (line 3))\n",
            "  Downloading https://files.pythonhosted.org/packages/28/5c/cf6a2b65a321c4a209efcdf64c2689efae2cb62661f8f6f4bb28547cf1bf/joblib-0.14.1-py2.py3-none-any.whl (294kB)\u001b[0m\n",
            "\u001b[34mCollecting regex (from nltk->-r requirements.txt (line 3))\n",
            "  Downloading https://files.pythonhosted.org/packages/09/c3/ddaa87500f31ed86290e3d014c0302a51fde28d7139eda0b5f33733726db/regex-2020.7.14.tar.gz (690kB)\u001b[0m\n",
            "\u001b[34mCollecting tqdm (from nltk->-r requirements.txt (line 3))\u001b[0m\n",
            "\u001b[34m  Downloading https://files.pythonhosted.org/packages/73/d5/f220e0c69b2f346b5649b66abebb391df1a00a59997a7ccf823325bd7a3e/tqdm-4.49.0-py2.py3-none-any.whl (69kB)\u001b[0m\n",
            "\u001b[34mCollecting soupsieve>1.2 (from beautifulsoup4->-r requirements.txt (line 4))\n",
            "  Downloading https://files.pythonhosted.org/packages/6f/8f/457f4a5390eeae1cc3aeab89deb7724c965be841ffca6cfca9197482e470/soupsieve-2.0.1-py3-none-any.whl\u001b[0m\n",
            "\u001b[34mRequirement already satisfied, skipping upgrade: six>=1.9 in /usr/local/lib/python3.5/dist-packages (from html5lib->-r requirements.txt (line 5)) (1.11.0)\u001b[0m\n",
            "\u001b[34mCollecting webencodings (from html5lib->-r requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/24/2a3e3df732393fed8b3ebf2ec078f05546de641fe1b667ee316ec1dcf3b7/webencodings-0.5.1-py2.py3-none-any.whl\u001b[0m\n",
            "\u001b[34mBuilding wheels for collected packages: nltk, train, regex\n",
            "  Running setup.py bdist_wheel for nltk: started\n",
            "  Running setup.py bdist_wheel for nltk: finished with status 'done'\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\u001b[0m\n",
            "\u001b[34m  Running setup.py bdist_wheel for train: started\n",
            "  Running setup.py bdist_wheel for train: finished with status 'done'\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-no7r_cac/wheels/35/24/16/37574d11bf9bde50616c67372a334f94fa8356bc7164af8ca3\n",
            "  Running setup.py bdist_wheel for regex: started\u001b[0m\n",
            "\n",
            "2020-09-18 03:57:57 Training - Training image download completed. Training in progress.\u001b[34m  Running setup.py bdist_wheel for regex: finished with status 'done'\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/55/dc/e17fa4568958f4c53be34b65e474a1327b64641f65df379ec3\u001b[0m\n",
            "\u001b[34mSuccessfully built nltk train regex\u001b[0m\n",
            "\u001b[34mInstalling collected packages: numpy, pytz, pandas, joblib, regex, tqdm, nltk, soupsieve, beautifulsoup4, webencodings, html5lib, train\n",
            "  Found existing installation: numpy 1.15.4\u001b[0m\n",
            "\u001b[34m    Uninstalling numpy-1.15.4:\n",
            "      Successfully uninstalled numpy-1.15.4\u001b[0m\n",
            "\u001b[34mSuccessfully installed beautifulsoup4-4.9.1 html5lib-1.1 joblib-0.14.1 nltk-3.5 numpy-1.18.5 pandas-0.24.2 pytz-2020.1 regex-2020.7.14 soupsieve-2.0.1 tqdm-4.49.0 train-1.0.0 webencodings-0.5.1\u001b[0m\n",
            "\u001b[34mYou are using pip version 18.1, however version 20.2.3 is available.\u001b[0m\n",
            "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
            "\u001b[34m2020-09-18 03:58:19,730 sagemaker-containers INFO     Invoking user script\n",
            "\u001b[0m\n",
            "\u001b[34mTraining Env:\n",
            "\u001b[0m\n",
            "\u001b[34m{\n",
            "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
            "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
            "    \"current_host\": \"algo-1\",\n",
            "    \"user_entry_point\": \"train.py\",\n",
            "    \"resource_config\": {\n",
            "        \"current_host\": \"algo-1\",\n",
            "        \"hosts\": [\n",
            "            \"algo-1\"\n",
            "        ],\n",
            "        \"network_interface_name\": \"eth0\"\n",
            "    },\n",
            "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
            "    \"model_dir\": \"/opt/ml/model\",\n",
            "    \"input_data_config\": {\n",
            "        \"training\": {\n",
            "            \"S3DistributionType\": \"FullyReplicated\",\n",
            "            \"TrainingInputMode\": \"File\",\n",
            "            \"RecordWrapperType\": \"None\"\n",
            "        }\n",
            "    },\n",
            "    \"job_name\": \"sagemaker-pytorch-2020-09-18-03-54-10-564\",\n",
            "    \"hyperparameters\": {\n",
            "        \"epochs\": 10,\n",
            "        \"hidden_dim\": 200\n",
            "    },\n",
            "    \"input_dir\": \"/opt/ml/input\",\n",
            "    \"log_level\": 20,\n",
            "    \"output_dir\": \"/opt/ml/output\",\n",
            "    \"additional_framework_parameters\": {},\n",
            "    \"network_interface_name\": \"eth0\",\n",
            "    \"hosts\": [\n",
            "        \"algo-1\"\n",
            "    ],\n",
            "    \"num_cpus\": 4,\n",
            "    \"module_name\": \"train\",\n",
            "    \"channel_input_dirs\": {\n",
            "        \"training\": \"/opt/ml/input/data/training\"\n",
            "    },\n",
            "    \"num_gpus\": 1,\n",
            "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
            "    \"module_dir\": \"s3://sagemaker-us-east-1-432646257739/sagemaker-pytorch-2020-09-18-03-54-10-564/source/sourcedir.tar.gz\"\u001b[0m\n",
            "\u001b[34m}\n",
            "\u001b[0m\n",
            "\u001b[34mEnvironment variables:\n",
            "\u001b[0m\n",
            "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
            "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
            "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
            "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
            "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
            "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
            "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
            "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
            "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
            "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
            "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_dim\":200},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"job_name\":\"sagemaker-pytorch-2020-09-18-03-54-10-564\",\"log_level\":20,\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-432646257739/sagemaker-pytorch-2020-09-18-03-54-10-564/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
            "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
            "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
            "\u001b[34mSM_HP_HIDDEN_DIM=200\u001b[0m\n",
            "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
            "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
            "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_dim\":200}\u001b[0m\n",
            "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
            "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
            "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
            "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-432646257739/sagemaker-pytorch-2020-09-18-03-54-10-564/source/sourcedir.tar.gz\u001b[0m\n",
            "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_dim\",\"200\"]\u001b[0m\n",
            "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
            "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
            "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
            "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
            "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/lib/python35.zip:/usr/lib/python3.5:/usr/lib/python3.5/plat-x86_64-linux-gnu:/usr/lib/python3.5/lib-dynload:/usr/local/lib/python3.5/dist-packages:/usr/lib/python3/dist-packages\n",
            "\u001b[0m\n",
            "\u001b[34mInvoking script with the following command:\n",
            "\u001b[0m\n",
            "\u001b[34m/usr/bin/python -m train --epochs 10 --hidden_dim 200\n",
            "\n",
            "\u001b[0m\n",
            "\u001b[34mUsing device cuda.\u001b[0m\n",
            "\u001b[34mGet train data loader.\u001b[0m\n",
            "\u001b[34mModel loaded with embedding_dim 32, hidden_dim 200, vocab_size 5000.\u001b[0m\n",
            "\u001b[34m2020-09-18 03:58:27,473 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
            "\n",
            "2020-09-18 03:58:35 Uploading - Uploading generated training model\n",
            "2020-09-18 03:58:35 Completed - Training job completed\n",
            "Training seconds: 105\n",
            "Billable seconds: 105\n"
          ]
        }
      ],
      "source": [
        "estimator.fit({'training': input_data})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_myWqSALGIsW"
      },
      "source": [
        "## Step 5: Testing the model and deploying the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTS-oTr3GIsX",
        "outputId": "c2269bbd-7bd5-4565-d552-ba917d4a364c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model.py  \u001b[0m\u001b[01;34m__pycache__\u001b[0m/  requirements.txt  train.py\r\n"
          ]
        }
      ],
      "source": [
        "ls train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0efKuNX_GIsX",
        "outputId": "909b7d0a-779a-479a-beab-cf95bd6786c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
            "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------!"
          ]
        }
      ],
      "source": [
        "# TODO: Deploy the trained model\n",
        "predictor = estimator.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD6wjvcTGIsX"
      },
      "source": [
        "## Step 7 - Use the model for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8W59RyIMGIsX"
      },
      "outputs": [],
      "source": [
        "test_X = pd.concat([pd.DataFrame(test_X_len), pd.DataFrame(test_X)], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20SS27XuGIsY"
      },
      "outputs": [],
      "source": [
        "# We split the data into chunks and send each chunk seperately, accumulating the results.\n",
        "\n",
        "def predict(data, rows=512):\n",
        "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
        "    predictions = np.array([])\n",
        "    for array in split_array:\n",
        "        predictions = np.append(predictions, predictor.predict(array))\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91PXemx9GIsY"
      },
      "outputs": [],
      "source": [
        "predictions = predict(test_X.values)\n",
        "predictions = [round(num) for num in predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xtqtix_lGIsY",
        "outputId": "0976f5d6-904f-4fb2-c9ec-0a8b3a2eb734"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.50324"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(test_y, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBmkKq31GIsZ"
      },
      "outputs": [],
      "source": [
        "test_review = 'The simplest pleasures in life are the best, and this film is one of them. Combining a rather basic storyline of love and adventure this movie transcends the usual weekend fair with wit and unmitigated charm.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PTcDZy4GIsZ"
      },
      "outputs": [],
      "source": [
        "# TODO: Convert test_review into a form usable by the model and save the results in test_data\n",
        "test_data = review_to_words(test_review)\n",
        "test_data = [np.array(convert_and_pad(word_dict, test_data)[0])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EFxU05WGIsa"
      },
      "source": [
        "Now that we have processed the review, we can send the resulting array to our model to predict the sentiment of the review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBWoYlS1GIsa",
        "outputId": "3961e176-2513-4fff-cb09-d70ad7db82d2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array(0.511356, dtype=float32)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictor.predict(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVRTHQ5qGIsa"
      },
      "source": [
        "### Delete the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDoF_fTQGIsa",
        "outputId": "a5acec44-f295-4ba0-dca7-c0fb6ce8af16"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "estimator.delete_endpoint() will be deprecated in SageMaker Python SDK v2. Please use the delete_endpoint() function on your predictor instead.\n"
          ]
        }
      ],
      "source": [
        "estimator.delete_endpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ds9y2YGGIsb",
        "outputId": "b5a7355b-e13b-4b7e-b7df-9acbc68ad0e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_containers\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mnn\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnn\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36moptim\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36moptim\u001b[39;49;00m\r\n",
            "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mutils\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mdata\u001b[39;49;00m\r\n",
            "\r\n",
            "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m LSTMClassifier\r\n",
            "\r\n",
            "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mutils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m review_to_words, convert_and_pad\r\n",
            "\r\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\r\n",
            "    \u001b[33m\"\"\"Load the PyTorch model from the `model_dir` directory.\"\"\"\u001b[39;49;00m\r\n",
            "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
            "\r\n",
            "    \u001b[37m# First, load the parameters used to create the model.\u001b[39;49;00m\r\n",
            "    model_info = {}\r\n",
            "    model_info_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel_info.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_info_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
            "        model_info = torch.load(f)\r\n",
            "\r\n",
            "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mmodel_info: \u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.format(model_info))\r\n",
            "\r\n",
            "    \u001b[37m# Determine the device and construct the model.\u001b[39;49;00m\r\n",
            "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
            "    model = LSTMClassifier(model_info[\u001b[33m'\u001b[39;49;00m\u001b[33membedding_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mhidden_dim\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], model_info[\u001b[33m'\u001b[39;49;00m\u001b[33mvocab_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
            "\r\n",
            "    \u001b[37m# Load the store model parameters.\u001b[39;49;00m\r\n",
            "    model_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mmodel.pth\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(model_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
            "        model.load_state_dict(torch.load(f))\r\n",
            "\r\n",
            "    \u001b[37m# Load the saved word_dict.\u001b[39;49;00m\r\n",
            "    word_dict_path = os.path.join(model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mword_dict.pkl\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(word_dict_path, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m f:\r\n",
            "        model.word_dict = pickle.load(f)\r\n",
            "\r\n",
            "    model.to(device).eval()\r\n",
            "\r\n",
            "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mDone loading model.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
            "    \u001b[34mreturn\u001b[39;49;00m model\r\n",
            "\r\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_fn\u001b[39;49;00m(serialized_input_data, content_type):\r\n",
            "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mDeserializing the input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mif\u001b[39;49;00m content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mtext/plain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
            "        data = serialized_input_data.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "        \u001b[34mreturn\u001b[39;49;00m data\r\n",
            "    \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRequested unsupported ContentType in content_type: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + content_type)\r\n",
            "\r\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_fn\u001b[39;49;00m(prediction_output, accept):\r\n",
            "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSerializing the generated output.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \u001b[34mreturn\u001b[39;49;00m \u001b[36mstr\u001b[39;49;00m(prediction_output)\r\n",
            "\r\n",
            "\u001b[34mdef\u001b[39;49;00m \u001b[32mpredict_fn\u001b[39;49;00m(input_data, model):\r\n",
            "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mInferring sentiment of input data.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "\r\n",
            "    device = torch.device(\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m torch.cuda.is_available() \u001b[34melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
            "    \r\n",
            "    \u001b[34mif\u001b[39;49;00m model.word_dict \u001b[35mis\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\r\n",
            "        \u001b[34mraise\u001b[39;49;00m \u001b[36mException\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mModel has not been loaded properly, no word_dict.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
            "    \r\n",
            "    \u001b[37m# TODO: Process input_data so that it is ready to be sent to our model.\u001b[39;49;00m\r\n",
            "    \u001b[37m#       You should produce two variables:\u001b[39;49;00m\r\n",
            "    \u001b[37m#         data_X   - A sequence of length 500 which represents the converted review\u001b[39;49;00m\r\n",
            "    \u001b[37m#         data_len - The length of the review\u001b[39;49;00m\r\n",
            "\r\n",
            "    \r\n",
            "    \r\n",
            "    \u001b[37m## first task 000000000000000000\u001b[39;49;00m\r\n",
            "    \r\n",
            "    \r\n",
            "    review_words = review_to_words(input_data)\r\n",
            "    review_words, length = convert_and_pad(model.word_dict, review_words)\r\n",
            "\r\n",
            "    data_X = review_words\r\n",
            "    data_len = length\r\n",
            "    \r\n",
            "    \r\n",
            "    \u001b[37m### 000000000000000000000\u001b[39;49;00m\r\n",
            "    \r\n",
            "\r\n",
            "    \u001b[37m# Using data_X and data_len we construct an appropriate input tensor. Remember\u001b[39;49;00m\r\n",
            "    \u001b[37m# that our model expects input data of the form 'len, review[500]'.\u001b[39;49;00m\r\n",
            "    data_pack = np.hstack((data_len, data_X))\r\n",
            "    data_pack = data_pack.reshape(\u001b[34m1\u001b[39;49;00m, -\u001b[34m1\u001b[39;49;00m)\r\n",
            "    \r\n",
            "    data = torch.from_numpy(data_pack)\r\n",
            "    data = data.to(device)\r\n",
            "\r\n",
            "    \u001b[37m# Make sure to put the model into evaluation mode\u001b[39;49;00m\r\n",
            "    model.eval()\r\n",
            "\r\n",
            "    \u001b[37m# TODO: Compute the result of applying the model to the input data. The variable `result` should\u001b[39;49;00m\r\n",
            "    \u001b[37m#       be a numpy array which contains a single integer which is either 1 or 0\u001b[39;49;00m\r\n",
            "    \r\n",
            "    \r\n",
            "    \u001b[37m## Second  task 000000000000000000\u001b[39;49;00m\r\n",
            "    \r\n",
            "    output = model(data).detach().cpu().numpy()\r\n",
            "    \u001b[36mprint\u001b[39;49;00m(output)\r\n",
            "\r\n",
            "    result = np.round(output).astype(np.int)\r\n",
            "\r\n",
            "    \u001b[34mreturn\u001b[39;49;00m result\r\n"
          ]
        }
      ],
      "source": [
        "!pygmentize serve/predict.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gndgk90QGIsc",
        "outputId": "56cd59b3-5882-45ab-e22e-8d7663493054"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
            "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------!"
          ]
        }
      ],
      "source": [
        "from sagemaker.predictor import RealTimePredictor\n",
        "from sagemaker.pytorch import PyTorchModel\n",
        "\n",
        "class StringPredictor(RealTimePredictor):\n",
        "    def __init__(self, endpoint_name, sagemaker_session):\n",
        "        super(StringPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='text/plain')\n",
        "\n",
        "model = PyTorchModel(model_data=estimator.model_data,\n",
        "                     role = role,\n",
        "                     framework_version='0.4.0',\n",
        "                     entry_point='predict.py',\n",
        "                     source_dir='serve',\n",
        "                     predictor_cls=StringPredictor)\n",
        "predictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8hXLUxYGIsc"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OwOtGAJGIsc"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "def test_reviews(data_dir='../data/aclImdb', stop=250):\n",
        "\n",
        "    results = []\n",
        "    ground = []\n",
        "\n",
        "    # We make sure to test both positive and negative reviews\n",
        "    for sentiment in ['pos', 'neg']:\n",
        "\n",
        "        path = os.path.join(data_dir, 'test', sentiment, '*.txt')\n",
        "        files = glob.glob(path)\n",
        "\n",
        "        files_read = 0\n",
        "\n",
        "        print('Starting ', sentiment, ' files')\n",
        "\n",
        "        # Iterate through the files and send them to the predictor\n",
        "        for f in files:\n",
        "            with open(f) as review:\n",
        "                # First, we store the ground truth (was the review positive or negative)\n",
        "                if sentiment == 'pos':\n",
        "                    ground.append(1)\n",
        "                else:\n",
        "                    ground.append(0)\n",
        "                # Read in the review and convert to 'utf-8' for transmission via HTTP\n",
        "                review_input = review.read().encode('utf-8')\n",
        "                # Send the review to the predictor and store the results\n",
        "                results.append(int(predictor.predict(review_input)))\n",
        "\n",
        "            # Sending reviews to our endpoint one at a time takes a while so we\n",
        "            # only send a small number of reviews\n",
        "            files_read += 1\n",
        "            if files_read == stop:\n",
        "                break\n",
        "\n",
        "    return ground, results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mL0xxxEGIsc",
        "outputId": "00c403f3-f7f7-4527-8aa1-5a73f309f582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting  pos  files\n",
            "Starting  neg  files\n"
          ]
        }
      ],
      "source": [
        "ground, results = test_reviews()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXgrmXi5GIsd",
        "outputId": "b0bb5c1e-0685-4333-8bfd-5d498c8dffd6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.494"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(ground, results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fE1xgoHGIsd",
        "outputId": "31c1952d-d61a-45de-c1d6-b1c67928b128"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'1'"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictor.predict(test_review)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEZvN1bKGIse"
      },
      "source": [
        "## Step 7 (again): Use the model for the web app\n",
        "\n",
        "### Setting up a Lambda function\n",
        "\n",
        "#### Part A: Create an IAM Role for the Lambda function\n",
        "\n",
        "#### Part B: Create a Lambda function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsVgu3s-GIse",
        "outputId": "1a6940ae-f43e-4e9b-d08a-d2e8878ad6d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'sagemaker-pytorch-2020-09-18-04-23-50-265'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictor.endpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qml403rTGIsf"
      },
      "source": [
        "### Delete the endpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xn_pKrrbGIsf"
      },
      "outputs": [],
      "source": [
        "predictor.delete_endpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-maoQUrGIsf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}